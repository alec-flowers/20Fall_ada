{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA - Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A) Tutorial\n",
    "\n",
    "In this tutorial, you will learn how to discover clusters in data using Python.\n",
    "\n",
    "Let's start by creating some **[synthetic](https://en.wikipedia.org/wiki/Synthetic_data)** data!\n",
    "First, we create a super secret number that represents the number of the cluster to generate. \n",
    "\n",
    "Don't print it! We'll try to discover it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "top_secret_number = random.randint(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate some data distributed in n groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 100\n",
    "\n",
    "# This create some artifical clusters with standard dev. = 2\n",
    "X, _, centers = make_blobs(n_samples=total_samples, \n",
    "                           centers=top_secret_number, \n",
    "                           cluster_std=2,\n",
    "                           n_features=2,\n",
    "                           return_centers=True, \n",
    "                           random_state=42)\n",
    "\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these clusters look like and where is their center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], alpha=0.6)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Artificial clusters (%s samples)\" % total_samples)\n",
    "\n",
    "for c in centers:\n",
    "    plt.scatter(c[0], c[1], marker=\"+\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data\n",
    "How many clusters do you see? Probably easy for a human, but not so trivial for a computer. \n",
    "\n",
    "Let's try to group the data with K-Means. Recall that K-Means requires you to specify the number of clusters (K). Let's start by testing multiple values between 2 and 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MIN_CLUSTERS = 2\n",
    "MAX_CLUSTERS = 10\n",
    "\n",
    "# Compute number of row and columns\n",
    "COLUMNS = 3\n",
    "ROWS = math.ceil((MAX_CLUSTERS-MIN_CLUSTERS)/COLUMNS)\n",
    "fig, axs = plt.subplots(ROWS, COLUMNS, figsize=(10,8), sharey=True, sharex=True)\n",
    "\n",
    "# Plot the clusters\n",
    "for n_clusters in range(MIN_CLUSTERS, MAX_CLUSTERS+1):\n",
    "    current_column = (n_clusters-MIN_CLUSTERS)%COLUMNS\n",
    "    current_row = (n_clusters-MIN_CLUSTERS)//COLUMNS\n",
    "    # Get the axis where to add the plot\n",
    "    ax = axs[current_row, current_column]\n",
    "    # Cluster the data with the current number of clusters\n",
    "    kmean = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "    # Plot the data by using the labels as color\n",
    "    ax.scatter(X[:,0], X[:,1], c=kmean.labels_, alpha=0.6)\n",
    "    ax.set_title(\"%s clusters\"%n_clusters)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    # Plot the centroids\n",
    "    for c in kmean.cluster_centers_:\n",
    "        ax.scatter(c[0], c[1], marker=\"+\", color=\"red\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus example: \n",
    "Did you know you can animate your plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "X_3clusters, _, centers = make_blobs(n_samples=100, \n",
    "                           centers=3, \n",
    "                           cluster_std=2,\n",
    "                           return_centers=True, \n",
    "                           random_state=99)\n",
    "\n",
    "# Update frame callback \n",
    "def update(max_iter):\n",
    "    # Clean the plot\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    # Cluster and get the labels\n",
    "    kmeans = KMeans(n_clusters=3, \n",
    "                    init=\"random\", \n",
    "                    algorithm=\"full\",\n",
    "                    random_state=10, \n",
    "                    n_init=1, max_iter=max_iter).fit(X_3clusters)\n",
    "    # Plot\n",
    "    plt.scatter(X_3clusters[:,0], X_3clusters[:,1], c=kmeans.labels_, alpha=0.6)\n",
    "    for c in kmeans.cluster_centers_:\n",
    "        plt.scatter(c[0], c[1], marker=\"+\", color=\"red\")\n",
    "    plt.title(\"%s iterations\" % max_iter)\n",
    "\n",
    "# Animate 1 frame per second\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(1, 20), interval=500)\n",
    "\n",
    "# Save the gif\n",
    "anim.save('clusters.gif', dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select K in K-Means?\n",
    "\n",
    "You have a couple of options:\n",
    "- Silhouette score: Find the K with the desired tradeoff between the number of clusters and cohesion/separation.\n",
    "- Elbow method: Find the \"elbow\" in the curve of the Sum of Squared Errors\n",
    "\n",
    "Let's see what we can observe using these 2 methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette score\n",
    "\n",
    "To generate the curve of the silhouette score, we cluster the data with different value of K and plot the resulting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(X)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method\n",
    "\n",
    "Similarly yo the previous case, we compute the SSE for different values of K, and we plot the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the two methods, we get different recommendations. Silhouette suggests that using 3 clusters is a fair tradeoff between the number of groups and their separation. The elbow method shows how the SSE reduction is less significant with more than 3 clusters.\n",
    "\n",
    "Let's plot the best clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(4,4), sharey=True)\n",
    "\n",
    "# Plot the clusters with K = 3\n",
    "labels = KMeans(n_clusters=3, random_state=0).fit_predict(X)\n",
    "axs.scatter(X[:,0], X[:,1], c=labels, alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately it is up to you (domain expert!) to pick the number of clusters that better represent the data.\n",
    "\n",
    "The best representation, in this case, is obtained with k = 3.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing high dimensional data\n",
    "\n",
    "Visualizing 2-dimensional data is easy, but what happens when we have more than three features? \n",
    "\n",
    "We can use dimensionality reduction techniques! Let start by generating some artifical clusters with 10 features. \n",
    "We use the same number of samples and groups as the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 100\n",
    "\n",
    "# This create some artifical clusters with standard dev. = 3\n",
    "X10d, _ = make_blobs(n_samples=total_samples, \n",
    "                           centers=top_secret_number, \n",
    "                           cluster_std=3,\n",
    "                           n_features=10,\n",
    "                           random_state=0)\n",
    "\n",
    "print(\"The features of the first sample are: %s\" % X10d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform this features vector in a form that can be represented in a simple plot, we can reduce the number of dimensions by preserving as much information as possible. Let's see two techniques that can help in this task:\n",
    "\n",
    "- t-SNE\n",
    "- PCA\n",
    "\n",
    "\n",
    "Note: the next lecture will cover more theoretical aspects of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne = TSNE(n_components=2, random_state=0).fit_transform(X10d)\n",
    "\n",
    "print(\"The features of the first sample are: %s\" % X_reduced_tsne[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca = PCA(n_components=2).fit(X10d).transform(X10d)\n",
    "\n",
    "print(\"The features of the first sample are: %s\" % X_reduced_pca[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are reduced in both cases to a 2d space. Please note that they are not the same because the two techniques optimize different objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(7,3), sharey=True)\n",
    "\n",
    "# Cluster the data in 3 groups\n",
    "labels = KMeans(n_clusters=3, random_state=0).fit_predict(X10d)\n",
    "\n",
    "# Plot the data reduced in 2d space with t-SNE\n",
    "axs[0].scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=labels, alpha=0.6)\n",
    "axs[0].set_title(\"t-SNE\")\n",
    "\n",
    "# Plot the data reduced in 2d space with PCA\n",
    "axs[1].scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=labels, alpha=0.6)\n",
    "axs[1].set_title(\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density based: DBSCAN\n",
    "\n",
    "K-Means is an intuitive and effective algorithm, but what happens when the data has a complex shape?\n",
    "\n",
    "As for the previous example, let's generate some synthetic data that resemble two half-moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 500 random points with 2-moons shape\n",
    "X_moons, _ = make_moons(500, noise=0.05, random_state=0)\n",
    "\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], alpha=0.6)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually see that there are two groups. Let's assign the labels with K-Means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(n_clusters=2, random_state=0).fit(X_moons)\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=kmean.labels_, alpha=0.6)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "# Plot the centroids\n",
    "for c in kmean.cluster_centers_:\n",
    "    plt.scatter(c[0], c[1], marker=\"o\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cluster the data with DBSCAN by variating the `eps` value in the range between 0.05 and 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of eps\n",
    "eps_list = np.linspace(0.05, 0.15, 14)\n",
    "\n",
    "# Compute number of row and columns\n",
    "COLUMNS = 7\n",
    "ROWS = math.ceil(len(eps_list)/COLUMNS)\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLUMNS, figsize=(12, 4), sharey=True, sharex=True)\n",
    "\n",
    "for i in range(0, len(eps_list)):\n",
    "    eps = eps_list[i]\n",
    "    \n",
    "    current_column = i%COLUMNS\n",
    "    current_row = i//COLUMNS\n",
    "    \n",
    "    ax = axs[current_row, current_column]\n",
    "    labels = DBSCAN(eps=eps).fit_predict(X_moons)\n",
    "    ax.scatter(X_moons[:,0], X_moons[:,1], c=labels, alpha=0.6)\n",
    "    ax.set_title(\"eps = {:.3f}\".format(eps))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Part B: Exercises\n",
    "\n",
    "## Exercise 1. Real world data! Wheat has a kernel too!\n",
    "\n",
    "\n",
    "<img src=\"img/wheat_banner.png\" width=\"800\">\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "> The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for\n",
    "the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/seeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = pd.read_csv(\"seeds_dataset.csv\")\n",
    "seeds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are interested in discovering structure in the data by hiding the actual label provided in the dataset.\n",
    "\n",
    "### Question 1.1: Prepare the dataset\n",
    "\n",
    "- Create a dataset by keeping only the meaningful features. Remove the type.\n",
    "- Plot the histogram of the different features.\n",
    "- The features have different scales, but we want to give the same importance to all them. Find a way to mitigate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Cluster the data with K-Means\n",
    "\n",
    "- Use the Elbow method to find the best value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Visualise your clusters\n",
    "\n",
    "- Use t-SNE to plot your clusters\n",
    "- Compare side by side 2 plots using the original labels (seedType) and the ones generated by K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Real world data! More examples with Wine\n",
    "\n",
    "> These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/wine\n",
    "\n",
    "### Question 2.1: Assign the wine to its group\n",
    "\n",
    "In this dataset the label is missing. Repeat the previous analysis to assign the wine to its original group.\n",
    "\n",
    "- Select the reasonable number of clusters\n",
    "- Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"wine-clustering.csv\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Part C: Quiz\n",
    "\n",
    "### Question 1: Which of the following real-world ML applications is not unsupervised learning?\n",
    "1. Netflix matrix factorization pipeline to discover users with similar interests\n",
    "2. Speaker recognition (recognition of the identity of who is talking) in phones and smart assistant devices\n",
    "3. LDA topic modeling on Twitter content to discover customers' opinions about a product\n",
    "4. K-means clustering of Web domains \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Which of the following is true for the k-means clustering algorithm?\n",
    "\n",
    "1. It always converges to a global optimum.\n",
    "2. In each iteration, points are assigned to the closest centroid and the new centroids are recomputed.\n",
    "3. It outputs the optimal number of clusters.\n",
    "4. None of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
